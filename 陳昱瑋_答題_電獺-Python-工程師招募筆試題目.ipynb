{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請輸入欲查詢「看板」(請參照README):MobileComm\n",
      "請輸入欲查詢「日期」(請參照README):2/23\n",
      "請輸入欲儲存「路徑」(請參照README):C:/Users/Sister/Desktop/\n"
     ]
    }
   ],
   "source": [
    "board = input(\"請輸入欲查詢「看板」(請參照README):\")\n",
    "want_date = input(\"請輸入欲查詢「日期」(請參照README):\")\n",
    "Access_path = input(\"請輸入欲儲存「路徑」(請參照README):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "web = 'https://www.ptt.cc/bbs/%s'%(board)+'/index.html'\n",
    "for p in range(3): #往上爬3頁\n",
    "    r = rq.get(web)\n",
    "    soup = BeautifulSoup(r.text,\"lxml\")\n",
    "    u = soup.select(\"div.btn-group.btn-group-paging a\") #a標籤\n",
    "    web = \"https://www.ptt.cc\"+ u[1][\"href\"] #取得上一頁的網址\n",
    "    \n",
    "#------將已被刪除的文章跳過------\n",
    "\n",
    "    main_content = rq.get(web)\n",
    "    main_content.text\n",
    "    main_soup = BeautifulSoup(main_content.text, \"lxml\") #使用lxml速度較快\n",
    "    url_list1 = main_soup.findAll('div', class_=\"title\") #取得各篇文章url\n",
    "    url_list2 = []\n",
    "    for t in url_list1:\n",
    "        a_item = t.select_one(\"a\")\n",
    "\n",
    "        if a_item:\n",
    "            b_url = a_item.get(\"href\")\n",
    "            url_list2.append(b_url)\n",
    "\n",
    "\n",
    "    date_list = main_soup.findAll('div', class_=\"date\") #取得各篇文章發文日期\n",
    "    date_list2 = []\n",
    "    for a in date_list:\n",
    "        date_list2.append(a.text) #將取得日期存進list，做後續步驟之日期條件判定\n",
    "\n",
    "#------取得該看板某頁面所有符合日期條件之內文資訊------\n",
    "\n",
    "    for b in range(0,len(url_list2)):\n",
    "        if date_list2[b] == \" \"+ want_date: #先判別文章發文日期是否為指定日期，是則進行內文資訊下載\n",
    "            get_url = 'https://www.ptt.cc' + url_list2[b]\n",
    "            content = rq.get(get_url)\n",
    "            content.text\n",
    "            soup = BeautifulSoup(content.text, \"lxml\")\n",
    "\n",
    "    #Step1.取得內文\n",
    "\n",
    "            def checkformat(soup, class_tag, data, index, get_url):\n",
    "                content_a = soup.select(class_tag)[index].text\n",
    "                return content_a\n",
    "\n",
    "            date = checkformat(soup, '.article-meta-value', 'date', 3, get_url)\n",
    "            content_a = soup.find(id=\"main-content\").text\n",
    "            target_content = u'※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "            #去除掉 target_content\n",
    "            content_a = content_a.split(target_content)\n",
    "            content_a = content_a[0].split(date)\n",
    "            #去除掉文末 --\n",
    "            main_content = content_a[1].replace('--', '')\n",
    "\n",
    "    #Step2.取得作者、看板、標題、發文時間\n",
    "\n",
    "            results = soup.select('span.article-meta-value')\n",
    "\n",
    "    #Step3.取得推文內容\n",
    "\n",
    "            url2 = 'https://www.ptt.cc/bbs/' + board\n",
    "            url3 = '.html'\n",
    "\n",
    "            number1 = get_url.replace(url2,\"\")\n",
    "            number2 = number1.replace(url3,\"\") #取得文章編號\n",
    "\n",
    "            message_id_list = soup.findAll('span', class_=\"push-userid\") #取得推文 ID\n",
    "            \n",
    "            message_id = []\n",
    "            for i in message_id_list:\n",
    "                message_id.append(i.text)\n",
    "\n",
    "            message_content_list = soup.findAll('span', class_=\"push-content\") #取得推文內容\n",
    "            #message_content_list\n",
    "            message_content = []\n",
    "            for j in message_content_list:\n",
    "                message_content.append(j.text)\n",
    "\n",
    "            message_time_list = soup.findAll('span', class_=\"push-ipdatetime\") #取得推文時間\n",
    "            #message_time_list\n",
    "            message_time = []\n",
    "            for k in message_time_list:\n",
    "                message_time.append(k.text)\n",
    "\n",
    "            ptt_dict1 = {\"推文作者\": message_id,\n",
    "                         \"推文內容\": message_content,\n",
    "                         \"推文時間\": message_time,\n",
    "                        }\n",
    "\n",
    "            df_ptt_dict1 = pd.DataFrame(ptt_dict1)\n",
    "\n",
    "            ptt_dict2 = {\"作者\": results[0].text,\n",
    "                        \"看板\": results[1].text,\n",
    "                        \"標題\": results[2].text,\n",
    "                        \"文章編號\": number2,\n",
    "                        \"時間\": results[3].text,\n",
    "                        \"內文\": main_content,\n",
    "                        \"網址\": get_url,\n",
    "                        }\n",
    "\n",
    "            df_ptt_dict2 = pd.DataFrame(ptt_dict2,index=[0])\n",
    "\n",
    "            final = pd.concat([df_ptt_dict2,df_ptt_dict1],axis=1)\n",
    "\n",
    "    #Step4.下載內文資訊至本機\n",
    "\n",
    "            final.to_csv(\"%s\"%(Access_path)+\"%s\"%(board)+\"_ptt_Crawler_data\"+str(b)+\".csv\", index=False, encoding='utf_8_sig') # 避免寫入csv產生亂碼\n",
    "\n",
    "            time.sleep(1)\n",
    "            #print(url_list[j][\"href\"])\n",
    "        else:\n",
    "            print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
